{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Lab 2: Code Review Assistant (Google Colab)\n",
    "\n",
    "In this lab you'll build a **Code Review Assistant** with an LLM. You'll:\n",
    "\n",
    "**What you'll learn**\n",
    "1. Set up SDK & helper for reliable LLM calls\n",
    "2. Review a single Python file for bugs, smells, and readability\n",
    "3. Generate **diff-style fixes** and **unit tests**\n",
    "4. Produce a **structured JSON report** with severities\n",
    "5. Run a **batch review** for multiple files (simulating a PR)\n",
    "\n",
    "> **Why are we doing this?**\n",
    "> Engineers use LLMs as *review bots* that flag issues and propose safe patches. You’ll learn stable prompts and JSON outputs you can wire into CI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Step 0 — Colab Runtime Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print('Python', sys.version)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    print('✅ Running in Google Colab')\n",
    "except Exception:\n",
    "    print('ℹ️ Not in Colab (that is okay for local runs).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔐 Step 1 — Install SDK & Set API Key\n",
    "\n",
    "We'll use the official **OpenAI Python SDK (>=1.0)**. It needs an API key.\n",
    "\n",
    "**How to use in Colab**\n",
    "1. Create a key at: https://platform.openai.com/account/api-keys\n",
    "2. Run the cell below (you’ll be prompted to paste the key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade openai>=1.40\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ or not os.environ['OPENAI_API_KEY']:\n",
    "    print('Enter your OpenAI API key (hidden):')\n",
    "    os.environ['OPENAI_API_KEY'] = getpass()\n",
    "print('✅ API key set in environment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧰 Step 2 — Minimal Client & Helpers\n",
    "\n",
    "We’ll create:\n",
    "- `call_llm` for general chat completions with retries\n",
    "- `call_llm_json` to enforce **strict JSON** outputs (for CI pipelines)\n",
    "- Prompt templates for **review**, **fix diff**, and **unit tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time, json, uuid\n",
    "from typing import List, Dict, Any\n",
    "\n", 
    "client = OpenAI()\n",
    "DEFAULT_MODEL = 'gpt-4o-mini'  # fast & cost-efficient for labs\n",
    "\n",
    "class LLMError(Exception):\n",
    "    pass\n",
    "\n",
    "def call_llm(messages: List[Dict[str, str]], model: str = DEFAULT_MODEL,\n",
    "             temperature: float = 0.2, max_tokens: int = 800,\n",
    "             retries: int = 2, **kwargs: Any) -> str:\n",
    "    last_err = None\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                **kwargs\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.6 * (attempt + 1))\n",
    "    raise LLMError(f'LLM call failed after retries: {last_err}')\n",
    "\n",
    "def call_llm_json(user_prompt: str, schema_hint: str, system_hint: str = None,\n",
    "                  temperature: float = 0.1, max_tokens: int = 1200) -> dict:\n",
    "    sys_msg = (system_hint or \"Return strictly valid JSON only. No markdown, no commentary.\")\n",
    "    text = call_llm([\n",
    "        {\"role\": \"system\", \"content\": sys_msg + \" Respond ONLY with JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Schema: {schema_hint}\\nInput: {user_prompt}\"}\n",
    "    ], temperature=temperature, max_tokens=max_tokens)\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Step 3 — A Buggy Python File (Demo Input)\n",
    "\n",
    "We’ll start with a small function that has multiple issues:\n",
    "- Arithmetic bug\n",
    "- Mutable default argument\n",
    "- Poor error handling\n",
    "- Missing docstring and type hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buggy_code = '''\\\n",
    "def accumulate(values=[]):\n",
    "    total = 0\n",
    "    for v in values:\n",
    "        total -= v  # BUG: should add\n",
    "    try:\n",
    "        avg = total / len(values)\n",
    "    except:\n",
    "        avg = 0\n",
    "    return total, avg\n",
    "'''\n",
    "print(buggy_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧑‍⚖️ Step 4 — Code Review Prompt (Findings + Severity)\n",
    "\n",
    "We’ll ask the model for a structured JSON report with severity for each issue. This is CI-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_schema = '{\"file\":\"str\",\"summary\":\"str\",\"issues\":[{\"title\":\"str\",\"severity\":\"oneof:LOW,MEDIUM,HIGH,CRITICAL\",\"category\":\"oneof:BUG,SECURITY,STYLE,PERF,MAINTAINABILITY\",\"line\":\"int|null\",\"explanation\":\"str\",\"suggestion\":\"str\"}]}'\n",
    "\n",
    "review_prompt = f\"\"\"\n",
    "Review the following Python source code. Identify bugs, security risks, style issues, performance concerns, and maintainability problems.\n",
    "Provide a JSON report that matches the given schema. Estimate the line number if possible.\n",
    "\n",
    "CODE:\\n{buggy_code}\n",
    "\"\"\"\n",
    "\n",
    "review = call_llm_json(user_prompt=review_prompt, schema_hint=review_schema, system_hint=(\n",
    "    \"You are a precise code review bot for Python.\\n\"\n",
    "    \"Explain issues clearly and suggest concrete changes.\"\n",
    "))\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🩹 Step 5 — Generate a Unified Diff Patch\n",
    "\n",
    "Ask for a minimal patch in **unified diff** format (compatible with `git apply`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_prompt = f\"\"\"\n",
    "Given this Python file, produce a minimal **unified diff** patch that fixes the issues while preserving function behavior.\n",
    "Only output the diff. Use filename `accumulate.py`.\n",
    "\n",
    "CODE:\\n{buggy_code}\n",
    "\"\"\"\n",
    "patch_text = call_llm([\n",
    "    {\"role\": \"system\", \"content\": \"You generate small, correct unified diffs. Output ONLY the diff.\"},\n",
    "    {\"role\": \"user\", \"content\": diff_prompt}\n",
    "], temperature=0.1, max_tokens=600)\n",
    "print(patch_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Step 6 — Generate Unit Tests (pytest)\n",
    "\n",
    "We’ll create minimal **pytest** tests using the intended behavior inferred from the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_prompt = f\"\"\"\n",
    "Write **pytest** unit tests for the fixed version of the function in `accumulate.py`.\n",
    "Cover: empty list, positive numbers, negative numbers, and mixed values. Use clear asserts.\n",
    "Return only Python test code (no explanations).\n",
    "\"\"\"\n",
    "tests_code = call_llm([\n",
    "    {\"role\": \"system\", \"content\": \"You write concise, correct pytest tests.\"},\n",
    "    {\"role\": \"user\", \"content\": tests_prompt}\n",
    "], temperature=0.2, max_tokens=700)\n",
    "print(tests_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧰 Step 7 — Wrap Into Reusable Functions\n",
    "\n",
    "These helpers let you plug any source string(s) and get a **report**, **patch**, and **tests** back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_source(filename: str, code_text: str) -> dict:\n",
    "    schema = '{\"file\":\"str\",\"summary\":\"str\",\"issues\":[{\"title\":\"str\",\"severity\":\"oneof:LOW,MEDIUM,HIGH,CRITICAL\",\"category\":\"oneof:BUG,SECURITY,STYLE,PERF,MAINTAINABILITY\",\"line\":\"int|null\",\"explanation\":\"str\",\"suggestion\":\"str\"}]}'\n",
    "    prompt = f\"Review file `{filename}` and return a JSON report matching schema.\\nCODE:\\n{code_text}\"\n",
    "    return call_llm_json(prompt, schema)\n",
    "\n",
    "def suggest_patch(filename: str, code_text: str) -> str:\n",
    "    prompt = f\"Produce a minimal unified diff to fix issues in `{filename}`. Output ONLY the diff.\\nCODE:\\n{code_text}\"\n",
    "    return call_llm([\n",
    "        {\"role\": \"system\", \"content\": \"You generate minimal, correct unified diffs.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], temperature=0.1, max_tokens=700)\n",
    "\n",
    "def generate_tests(filename: str, code_text: str) -> str:\n",
    "    prompt = (\n",
    "        f\"Write pytest tests for the fixed version of `{filename}` covering typical and edge cases. \"\n",
    "        \"Return only code.\"\n",
    "    )\n",
    "    return call_llm([\n",
    "        {\"role\": \"system\", \"content\": \"You write concise, correct pytest tests.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], temperature=0.2, max_tokens=800)\n",
    "\n",
    "# Quick sanity check on helpers\n",
    "report = review_source('accumulate.py', buggy_code)\n",
    "patch  = suggest_patch('accumulate.py', buggy_code)\n",
    "tests  = generate_tests('accumulate.py', buggy_code)\n",
    "print(report['summary'])\n",
    "print('\\n--- DIFF ---\\n', patch[:400], '...')\n",
    "print('\\n--- TESTS ---\\n', tests[:400], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Step 8 — Batch Review (Simulated PR)\n",
    "\n",
    "Feed a dict of `{filename: code}` and get a combined review with per-file issues and a summary. This simulates a **pull request** code review workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pr = {\n",
    "    'accumulate.py': buggy_code,\n",
    "    'utils.py': '''\\\n",
    "def is_even(n):\n",
    "    # style issue: no typing, no docstring\n",
    "    return n % 2 == 0\n",
    "'''\n",
    "}\n",
    "\n",
    "def review_pr(files: Dict[str, str]) -> dict:\n",
    "    items = []\n",
    "    for fname, code in files.items():\n",
    "        items.append({\"file\": fname, \"code\": code})\n",
    "    schema = '{\"summary\":\"str\",\"files\":[{\"file\":\"str\",\"issues\":[{\"title\":\"str\",\"severity\":\"oneof:LOW,MEDIUM,HIGH,CRITICAL\",\"category\":\"oneof:BUG,SECURITY,STYLE,PERF,MAINTAINABILITY\",\"line\":\"int|null\",\"explanation\":\"str\",\"suggestion\":\"str\"}]}]}'\n",
    "    user_prompt = json.dumps({\"pr\": items})\n",
    "    return call_llm_json(user_prompt, schema, system_hint=(\n",
    "        \"You are a senior code reviewer. Aggregate issues across files and produce a concise summary.\"\n",
    "    ))\n",
    "\n",
    "pr_report = review_pr(sample_pr)\n",
    "pr_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 What to try next\n",
    "\n",
    "1. Swap `buggy_code` with your own file(s) — paste code or read from GitHub.\n",
    "2. Tighten prompts for your org’s **style guide** and **security baselines**.\n",
    "3. Have the bot return **SARIF** for integration with code scanning.\n",
    "4. Post results to a PR via CI (e.g., GitHub Actions) and gate on `CRITICAL` issues.\n",
    "\n",
    "**You’ve completed Lab 2.** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
