{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Lab 4: Fine-Tuning Demo â€” Google Colab\n",
    "\n",
    "This lab demonstrates **two practical ways** to fine-tune an LLM for software-engineering tasks:\n",
    "\n",
    "1. **Option A (OpenAI SFT)**: Supervised fine-tuning via the OpenAI API using a tiny JSONL dataset.\n",
    "2. **Option B (Local LoRA)**: Parameter-efficient fine-tuning (LoRA) on a small open model (`distilgpt2`) using Hugging Face PEFT on Colab GPU.\n",
    "\n",
    "### What you'll learn\n",
    "- When to pick Fine-Tuning vs RAG\n",
    "- How to prepare **instruction JSONL** datasets\n",
    "- How to launch a **fine-tuning job** (OpenAI) and **consume** the tuned model\n",
    "- How to **train adapters** locally with **LoRA** and generate improved outputs\n",
    "\n",
    "> **Why are we doing this?**\n",
    "> Fine-tuning is ideal to enforce **style, tone, or constrained behavior** (e.g., your team's docstring format). RAG is better for **fresh facts**. In practice, teams often combine both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Step 0 â€” Colab Runtime & (Optional) GPU\n",
    "GPU helps for Option B (LoRA). For Option A (OpenAI SFT), CPU is fine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys, platform\\n",
    "print('Python:', sys.version)\\n",
    "try:\\n",
    "    import google.colab  # type: ignore\\n",
    "    print('âœ… Running in Google Colab')\\n",
    "except Exception:\\n",
    "    print('â„¹ï¸ Not in Colab (that is okay).')\\n",
    "try:\\n",
    "    import torch\\n",
    "    print('Torch:', torch.__version__)\\n",
    "    print('CUDA available:', torch.cuda.is_available())\\n",
    "    if torch.cuda.is_available():\\n",
    "        print('GPU:', torch.cuda.get_device_name(0))\\n",
    "except Exception as e:\\n",
    "    print('Torch not available yet:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ” Fine-Tuning vs RAG (1 slide recap)\n",
    "- **RAG**: Best for *factual grounding* from your knowledge base; no weight changes.\\n",
    "- **Fine-Tuning**: Best for *style, format, persona, guardrails*; changes model behavior on patterns you show.\n",
    "\n",
    "In this lab weâ€™ll do **both paths** so you can adopt whichever fits your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## â–¶ï¸ Option A â€” OpenAI Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Weâ€™ll create a **tiny** instruction dataset (JSONL), upload it, start a fine-tune job, poll status, and then use the tuned model.\n",
    "\n",
    "**Prereqs:** OpenAI account & API key; model access that supports fine-tuning.\n",
    "\n",
    "**Note:** This is a minimal demo; real datasets require more examples (~100â€“1000+) and review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Step A1 â€” Install SDK & Load API Key"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip -q install --upgrade openai>=1.40\\n",
    "\\n",
    "import os, json, time\\n",
    "from getpass import getpass\\n",
    "from openai import OpenAI\\n",
    "\\n",
    "if 'OPENAI_API_KEY' not in os.environ or not os.environ['OPENAI_API_KEY']:\\n",
    "    print('Enter your OpenAI API key (hidden):')\\n",
    "    os.environ['OPENAI_API_KEY'] = getpass()\\n",
    "print('âœ… API key set in environment.')\\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§± Step A2 â€” Create a Tiny Instruction Dataset (JSONL)\n",
    "\n",
    "Each line is a training example with `messages` (system/user/assistant). Keep it **consistent** and **on-style**. Below we bias for **Python-docstring, concise, testable** outputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = [\\n",
    "  {\"messages\": [\\n",
    "    {\"role\": \"system\", \"content\": \"You are a senior Python tutor. Use docstrings, type hints, and short examples.\"},\\n",
    "    {\"role\": \"user\", \"content\": \"Write a function that returns the Fibonacci sequence up to n.\"},\\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"\\n",
    "def fibonacci(n: int) -> list:\\n",
    "    \"\"\"Return Fibonacci numbers from 0 up to n (inclusive).\"\"\"\\n",
    "    a, b, out = 0, 1, []\\n",
    "    while a <= n:\\n",
    "        out.append(a)\\n",
    "        a, b = b, a + b\\n",
    "    return out\\n",
    "\"\"\"}\\n",
    "  ]},\\n",
    "  {\"messages\": [\\n",
    "    {\"role\": \"system\", \"content\": \"You are a senior Python tutor. Use docstrings, type hints, and short examples.\"},\\n",
    "    {\"role\": \"user\", \"content\": \"Explain Python decorators with one short example.\"},\\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"\\n",
    "\"\"\"Decorators wrap a function to add behavior without changing its code.\"\"\"\\n",
    "def log_calls(fn):\\n",
    "    def wrapper(*args, **kwargs):\\n",
    "        print(f\"calling {fn.__name__}\")\\n",
    "        return fn(*args, **kwargs)\\n",
    "    return wrapper\\n",
    "\\n",
    "@log_calls\\n",
    "def hello(name: str) -> str:\\n",
    "    return f\"Hello {name}\"\\n",
    "\"\"\"}\\n",
    "  ]},\\n",
    "  {\"messages\": [\\n",
    "    {\"role\": \"system\", \"content\": \"You are a senior Python tutor. Use docstrings, type hints, and short examples.\"},\\n",
    "    {\"role\": \"user\", \"content\": \"Show a robust average function that avoids ZeroDivisionError and handles None.\"},\\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"\\n",
    "from statistics import mean\\n",
    "from typing import Iterable, Optional\\n",
    "\\n",
    "def safe_avg(xs: Optional[Iterable[float]]) -> float:\\n",
    "    \"\"\"Return the average; 0.0 if xs is None/empty.\"\"\"\\n",
    "    xs = list(x for x in (xs or []) if x is not None)\\n",
    "    return mean(xs) if xs else 0.0\\n",
    "\"\"\"}\\n",
    "  ]}\\n",
    "]\\n",
    "\\n",
    "jsonl_path = 'tiny_python_tutor.jsonl'\\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f:\\n",
    "    for row in dataset:\\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + '\\n')\\n",
    "print('Wrote', jsonl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â¬†ï¸ Step A3 â€” Upload File & Start Fine-Tune Job\n",
    "\n",
    "- `purpose` must be `'fine-tune'` for training data.\\n",
    "- Use an available base model that supports SFT. Here we use a small, cost-efficient model as a placeholder (adjust to what your account supports)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "ft_base_model = 'gpt-4o-mini'  # adjust to a fine-tune-able model available to your account\\n",
    "\\n",
    "file_obj = client.files.create(\\n",
    "    file=open(jsonl_path, 'rb'),\\n",
    "    purpose='fine-tune'\\n",
    ")\\n",
    "print('Uploaded file id:', file_obj.id)\\n",
    "\\n",
    "job = client.fine_tuning.jobs.create(\\n",
    "    training_file=file_obj.id,\\n",
    "    model=ft_base_model,\\n",
    "    # optional: validation_file=..., suffix='python-tutor-style', hyperparameters={...}\\n",
    ")\\n",
    "job_id = job.id\\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â³ Step A4 â€” Poll Job Status\n",
    "Weâ€™ll check the job until it completes/failed (this is just a helper loop for the demo)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\\n",
    "def wait_for_job(jid, sleep_s=10, max_wait_s=1200):\\n",
    "    start = time.time()\\n",
    "    while True:\\n",
    "        j = client.fine_tuning.jobs.retrieve(jid)\\n",
    "        print('status:', j.status)\\n",
    "        if j.status in ('succeeded', 'failed', 'cancelled'):\\n",
    "            return j\\n",
    "        if time.time() - start > max_wait_s:\\n",
    "            print('â±ï¸ Timed out waiting. Check job in dashboard.')\\n",
    "            return j\\n",
    "        time.sleep(sleep_s)\\n",
    "\\n",
    "final = wait_for_job(job_id, sleep_s=15)\\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– Step A5 â€” Use the Fine-Tuned Model\n",
    "If `status = succeeded`, a `fine_tuned_model` name should be available. Use it like any other chat model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "ft_model = getattr(final, 'fine_tuned_model', None) or getattr(final, 'result', None)\\n",
    "print('Fine-tuned model:', ft_model)\\n",
    "if ft_model:\\n",
    "    resp = client.chat.completions.create(\\n",
    "        model=ft_model,\\n",
    "        messages=[\\n",
    "            {\"role\": \"user\", \"content\": \"Write a Python function to compute factorial with docstring and type hints.\"}\\n",
    "        ],\\n",
    "        temperature=0.2,\\n",
    "        max_tokens=350\\n",
    "    )\\n",
    "    print(resp.choices[0].message.content)\\n",
    "else:\\n",
    "    print('âš ï¸ Fine-tuned model not available yet. Check job status or dashboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## â–¶ï¸ Option B â€” Local LoRA (PEFT) on Colab GPU\n",
    "\n",
    "Weâ€™ll fine-tune a small open model (`distilgpt2`) with **LoRA adapters** using PEFT. This is quick & cheap and demonstrates the mechanics of SFT without external services.\n",
    "\n",
    "**Tip**: For faster runs, keep dataset tiny and epochs low. For quality, expand dataset and tune hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Step B1 â€” Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip -q install --upgrade transformers datasets accelerate peft bitsandbytes\\n",
    "import torch, os\\n",
    "print('CUDA available:', torch.cuda.is_available())\\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§± Step B2 â€” Tiny Instruction Dataset (Same Intent as Option A)\n",
    "Weâ€™ll build a very small list of instruction/response pairs to bias `distilgpt2` towards **Python-tutor** style outputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_pairs = [\\n",
    "    {\"instruction\": \"Write fibonacci(n) with docstring and type hints.\",\\n",
    "     \"response\": \"\"\"\\n",
    "def fibonacci(n: int) -> list:\\n",
    "    \"\"\"Return Fibonacci numbers from 0 up to n (inclusive).\"\"\"\\n",
    "    a, b, out = 0, 1, []\\n",
    "    while a <= n:\\n",
    "        out.append(a)\\n",
    "        a, b = b, a + b\\n",
    "    return out\\n",
    "\"\"\"},\\n",
    "    {\"instruction\": \"Explain Python decorators with a short example.\",\\n",
    "     \"response\": \"\"\"\\n",
    "\"\"\"Decorators wrap a function to add behavior without changing its code.\"\"\"\\n",
    "def log_calls(fn):\\n",
    "    def wrapper(*args, **kwargs):\\n",
    "        print(f\"calling {fn.__name__}\")\\n",
    "        return fn(*args, **kwargs)\\n",
    "    return wrapper\\n",
    "\\n",
    "@log_calls\\n",
    "def hello(name: str) -> str:\\n",
    "    return f\"Hello {name}\"\\n",
    "\"\"\"},\\n",
    "    {\"instruction\": \"Provide a safe average function handling empty list and None.\",\\n",
    "     \"response\": \"\"\"\\n",
    "from statistics import mean\\n",
    "from typing import Iterable, Optional\\n",
    "def safe_avg(xs: Optional[Iterable[float]]) -> float:\\n",
    "    \"\"\"Return the average; 0.0 if xs is None/empty.\"\"\"\\n",
    "    xs = list(x for x in (xs or []) if x is not None)\\n",
    "    return mean(xs) if xs else 0.0\\n",
    "\"\"\"}\\n",
    "]\\n",
    "len(train_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¤ Step B3 â€” Tokenization & Formatting\n",
    "Weâ€™ll build a simple prompt template: `Instruction:\\n...\\nResponse:\\n...` and train the model to produce the **response** when given the **instruction** prefix."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\\n",
    "from datasets import Dataset\\n",
    "\\n",
    "base_model = 'distilgpt2'\\n",
    "tok = AutoTokenizer.from_pretrained(base_model)\\n",
    "if tok.pad_token is None:\\n",
    "    tok.pad_token = tok.eos_token\\n",
    "\\n",
    "def build_text(rec):\\n",
    "    return f\"Instruction:\\n{rec['instruction']}\\n\\nResponse:\\n{rec['response']}\"\\n",
    "\\n",
    "ds = Dataset.from_list([{\"text\": build_text(x)} for x in train_pairs])\\n",
    "\\n",
    "def tok_fn(batch):\\n",
    "    out = tok(batch['text'], truncation=True, padding='max_length', max_length=512)\\n",
    "    out['labels'] = out['input_ids'].copy()\\n",
    "    return out\\n",
    "\\n",
    "ds_tok = ds.map(tok_fn, batched=True, remove_columns=['text'])\\n",
    "ds_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Step B4 â€” LoRA Configuration & Trainer\n",
    "We apply LoRA adapters to a few attention modules and train for a couple epochs (very quick)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\\n",
    "from transformers import TrainingArguments, Trainer\\n",
    "\\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model).to(device)\\n",
    "\\n",
    "peft_cfg = LoraConfig(\\n",
    "    task_type=TaskType.CAUSAL_LM,\\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\\n",
    "    target_modules=['c_attn', 'q_attn'] if any('q_attn' in n for n,_ in model.named_modules()) else ['c_attn']\\n",
    ")\\n",
    "model = get_peft_model(model, peft_cfg)\\n",
    "model.print_trainable_parameters()\\n",
    "\\n",
    "args = TrainingArguments(\\n",
    "    output_dir='out-lora',\\n",
    "    per_device_train_batch_size=2,\\n",
    "    num_train_epochs=2,\\n",
    "    learning_rate=2e-4,\\n",
    "    fp16=torch.cuda.is_available(),\\n",
    "    gradient_accumulation_steps=2,\\n",
    "    logging_steps=5,\\n",
    "    save_strategy='no'\\n",
    ")\\n",
    "\\n",
    "trainer = Trainer(\\n",
    "    model=model,\\n",
    "    args=args,\\n",
    "    train_dataset=ds_tok\\n",
    ")\\n",
    "trainer.train()\\n",
    "model.to('cpu').save_pretrained('out-lora/model')\\n",
    "tok.save_pretrained('out-lora/tokenizer')\\n",
    "print('âœ… Saved LoRA adapter and tokenizer to out-lora/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª Step B5 â€” Inference with the LoRA-tuned Model\n",
    "Generate responses in the new **Python-tutor** style."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import PeftModel\\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\\n",
    "\\n",
    "tok = AutoTokenizer.from_pretrained('out-lora/tokenizer')\\n",
    "base = AutoModelForCausalLM.from_pretrained(base_model)\\n",
    "model = PeftModel.from_pretrained(base, 'out-lora/model').to(device)\\n",
    "\\n",
    "def generate(prompt, max_new_tokens=180, temperature=0.2):\\n",
    "    text = f\"Instruction:\\n{prompt}\\n\\nResponse:\\n\"\\n",
    "    ids = tok(text, return_tensors='pt').to(device)\\n",
    "    out = model.generate(**ids, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, pad_token_id=tok.eos_token_id)\\n",
    "    return tok.decode(out[0], skip_special_tokens=True).split('Response:\\n', 1)[-1]\\n",
    "\\n",
    "print(generate('Write factorial(n) with docstring and type hints.'))\\n",
    "print('\\n---\\n')\\n",
    "print(generate('Explain Python context managers with a tiny example.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Wrap-Up & Next Steps\n",
    "You built two **fine-tuning** paths:\n",
    "- **OpenAI SFT**: Upload JSONL â†’ fine-tune â†’ use tuned model.\n",
    "- **Local LoRA**: Train lightweight adapters on `distilgpt2` and generate tutor-style code.\n",
    "\n",
    "### What to try next\n",
    "1. Expand your dataset to 100â€“1000+ high-quality examples (consistent tone & formatting).\n",
    "2. Add **evals** (prompted unit tests, BLEU/ROUGE for style tasks) to measure gains.\n",
    "3. Mix with **RAG** for factual grounding + fine-tuned style.\n",
    "4. For LoRA: try bigger backbones (e.g., `gpt2`, `TinyLlama`), more steps, and better prompt templates.\n",
    "5. For OpenAI: add **validation_file**, **suffix**, and **hyperparameters**; track results and iterate.\n",
    "\n",
    "**Youâ€™ve completed Lab 4. ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
