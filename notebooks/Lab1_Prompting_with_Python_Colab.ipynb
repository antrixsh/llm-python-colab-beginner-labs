{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Lab 1: Prompting an LLM with Python (Google Colab)\n",
    "\n",
    "This lab shows how to call a Large Language Model (LLM) from Python, craft better prompts, and control generation using parameters like **temperature** and **max_tokens**.\n",
    "\n",
    "### What you'll learn\n",
    "1. How to set up environment and API keys in Colab\n",
    "2. How to send your first prompt\n",
    "3. How to improve prompts (role, constraints, few-shot examples)\n",
    "4. How decoding parameters affect output\n",
    "5. How to wrap calls in a reusable helper with retries\n",
    "\n",
    "> **Why are we doing this?**\n",
    "> In real engineering, LLMs act like an *API-first component*. Mastering prompts & parameters is the quickest way to get reliable, repeatable behavior for prototypes and production services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Step 0 — Colab Runtime Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print('Python', sys.version)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    print('✅ Running in Google Colab')\n",
    "except Exception:\n",
    "    print('ℹ️ Not in Colab (that is okay for local runs).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔐 Step 1 — Install SDK & Set API Key\n",
    "\n",
    "We will use the official **OpenAI Python SDK (>=1.0)**. You need an API key.\n",
    "\n",
    "**In Colab:**\n",
    "1. Go to [OpenAI API Keys](https://platform.openai.com/account/api-keys) and create a key.\n",
    "2. Run the cell below — it will prompt you to paste the key.\n",
    "\n",
    "We store the key in an environment variable to avoid hard-coding secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade openai>=1.40 matplotlib\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ or not os.environ['OPENAI_API_KEY']:\n",
    "    print('Enter your OpenAI API key (it will be hidden):')\n",
    "    os.environ['OPENAI_API_KEY'] = getpass()\n",
    "\n",
    "print('✅ API key loaded into environment (not printed).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧰 Step 2 — Minimal Client & Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "DEFAULT_MODEL = 'gpt-4o-mini'  # fast & cost-efficient for labs\n",
    "\n",
    "class LLMError(Exception):\n",
    "    pass\n",
    "\n",
    "def call_llm(messages: List[Dict[str, str]],\n",
    "             model: str = DEFAULT_MODEL,\n",
    "             temperature: float = 0.7,\n",
    "             max_tokens: int = 400,\n",
    "             retries: int = 2,\n",
    "             **kwargs: Any) -> str:\n",
    "    last_err = None\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                **kwargs\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.6 * (attempt + 1))\n",
    "    raise LLMError(f'LLM call failed after retries: {last_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Step 3 — Your First Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_llm([\n",
    "    {\"role\": \"user\", \"content\": \"Explain Python decorators in simple terms with a short example.\"}\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎭 Step 4 — Add a System Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_llm([\n",
    "    {\"role\": \"system\", \"content\": \"You are a senior Python instructor. Be concise and precise.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain Python generators with a tiny example.\"}\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📐 Step 5 — Add Constraints & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Explain the Strategy design pattern for Python.\\n\"\n",
    "    \"Return exactly 3 bullet points and a 6-line code example.\"\n",
    ")\n",
    "response = call_llm([\n",
    "    {\"role\": \"system\", \"content\": \"You are a precise software architect.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Step 6 — Few-Shot Prompting (Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You write compact, idiomatic Python.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Task: Write a function with docstring that reverses a string.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "\"\"\"Utilities for strings\"\"\"\n",
    "\n",
    "def reverse_text(s: str) -> str:\n",
    "    \"\"\"Return the reverse of input string.\"\"\"\n",
    "    return s[::-1]\n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"Task: Write a function with docstring that counts vowels in a string.\"}\n",
    "]\n",
    "\n",
    "print(call_llm(messages, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎚️ Step 7 — Decoding Parameters: Temperature & Max Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give a one-sentence metaphor for microservices.\"\n",
    "for t in [0.0, 0.4, 0.9]:\n",
    "    txt = call_llm([\n",
    "        {\"role\": \"system\", \"content\": \"Be vivid but concise.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ], temperature=t, max_tokens=60)\n",
    "    print(f\"\\nTemperature={t}:\\n{txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧱 Step 8 — Robust Wrapper with JSON Output (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, json\n",
    "\n",
    "def call_llm_json(prompt: str, schema_hint: str) -> dict:\n",
    "    trace_id = str(uuid.uuid4())\n",
    "    sys_msg = (\n",
    "        \"You are a service that returns strictly valid JSON per the user's schema hint.\"\n",
    "        f\" Always return ONLY JSON. trace_id={trace_id}\"\n",
    "    )\n",
    "    text = call_llm([\n",
    "        {\"role\": \"system\", \"content\": sys_msg},\n",
    "        {\"role\": \"user\", \"content\": f\"Schema: {schema_hint}\\nInput: {prompt}\"}\n",
    "    ], temperature=0.2, max_tokens=350)\n",
    "    return json.loads(text)\n",
    "\n",
    "print(call_llm_json(\n",
    "    prompt=\"Generate 3 test cases for a login API with fields email and password.\",\n",
    "    schema_hint='{\"tests\":[{\"name\": \"str\", \"input\": {\"email\": \"str\", \"password\": \"str\"}}]}'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 What to try next\n",
    "\n",
    "1. Swap the task (e.g., summarize logs, propose test cases, write docstrings).\n",
    "2. Change **temperature** and compare tone/variety.\n",
    "3. Add **few-shot** examples for your team's preferred format.\n",
    "4. Wrap `call_llm` with observability and rate-limit handling.\n",
    "\n",
    "**You’ve completed Lab 1.** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
